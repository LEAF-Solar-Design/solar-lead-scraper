# Codebase Structure

**Analysis Date:** 2026-01-18

## Directory Layout

```
solar-lead-scraper/
├── .claude/                    # Claude Code local settings
│   └── settings.local.json     # IDE configuration
├── .github/                    # GitHub configuration
│   └── workflows/              # GitHub Actions workflows
│       └── scrape-leads.yml    # Daily scrape + upload workflow
├── .planning/                  # Planning and analysis documents
│   └── codebase/               # Codebase analysis (this file)
├── output/                     # Generated CSV files (gitignored)
│   └── solar_leads_*.csv       # Timestamped lead exports
├── .gitignore                  # Git ignore rules
├── requirements.txt            # Python dependencies
├── scraper.py                  # Main scraping and filtering logic
└── upload_results.py           # Dashboard upload utility
```

## Directory Purposes

**`.github/workflows/`:**
- Purpose: CI/CD automation definitions
- Contains: GitHub Actions YAML workflow files
- Key files: `scrape-leads.yml` (daily automated scrape)

**`output/`:**
- Purpose: Generated CSV output from scraper runs
- Contains: Timestamped CSV files with lead data
- Key files: `solar_leads_{YYYYMMDD}_{HHMMSS}.csv`
- Note: Directory is gitignored; files only persist locally or as GH Actions artifacts

**`.planning/codebase/`:**
- Purpose: Architecture and analysis documentation
- Contains: Markdown analysis files generated by codebase mapping
- Key files: `ARCHITECTURE.md`, `STRUCTURE.md`

## Key File Locations

**Entry Points:**
- `scraper.py`: Primary scraping script - run to generate leads
- `upload_results.py`: Secondary upload script - run after scraping to push to dashboard

**Configuration:**
- `requirements.txt`: Python package dependencies (python-jobspy, pandas)
- `.github/workflows/scrape-leads.yml`: Automated workflow configuration
- `.gitignore`: Files excluded from version control

**Core Logic:**
- `scraper.py` lines 71-221: `description_matches()` - 6-tier filtering logic
- `scraper.py` lines 224-325: `scrape_solar_jobs()` - search term iteration and job fetching
- `scraper.py` lines 328-366: `process_jobs()` - deduplication and enrichment
- `scraper.py` lines 15-48: LinkedIn URL generators

**Output:**
- `output/solar_leads_*.csv`: Generated lead files with company data

## Naming Conventions

**Files:**
- Python modules: `snake_case.py` (e.g., `scraper.py`, `upload_results.py`)
- Output files: `snake_case_{timestamp}.csv` (e.g., `solar_leads_20260115_231132.csv`)
- Config files: Standard names (e.g., `requirements.txt`, `.gitignore`)

**Functions:**
- All functions use `snake_case`: `scrape_solar_jobs()`, `description_matches()`, `clean_company_name()`
- Generator functions prefixed with `generate_`: `generate_linkedin_search_url()`
- Boolean filter functions end with verb: `description_matches()`

**Variables:**
- Local variables: `snake_case` (e.g., `all_jobs`, `clean_name`, `desc_lower`)
- Constants: Inline lists, no separate constants file
- DataFrame columns: `snake_case` (e.g., `company`, `job_title`, `posting_url`)

**Directories:**
- Lowercase with hyphens for project root: `solar-lead-scraper`
- Lowercase single words for subdirs: `output`
- Dot-prefixed for special dirs: `.github`, `.planning`, `.claude`

## Where to Add New Code

**New Search Terms:**
- Location: `scraper.py` lines 229-287 in `search_terms` list
- Pattern: Add string to existing list

**New Filter Criteria:**
- Exclusion terms: Add to relevant `*_terms` lists in `description_matches()` (lines 86-150)
- Inclusion tiers: Add new tier logic following existing pattern (lines 153-220)

**New LinkedIn Search Type:**
- Location: `scraper.py` after line 48
- Pattern: Create `generate_linkedin_{type}_search_url(company_name: str)` function
- Integration: Add column in `process_jobs()` lines 357-360

**New Output Column:**
- Location: `scraper.py` in `process_jobs()` function
- Add transformation logic, then add column name to `final_columns` list (line 363)

**New Utility Script:**
- Location: Root directory as new `*.py` file
- Pattern: Include `if __name__ == "__main__": main()` entry point
- Integration: Add step to `.github/workflows/scrape-leads.yml` if automated

**New Workflow:**
- Location: `.github/workflows/` directory
- Pattern: New YAML file following existing `scrape-leads.yml` structure

## Special Directories

**`output/`:**
- Purpose: Runtime-generated CSV lead files
- Generated: Yes, by scraper.py
- Committed: No (gitignored)
- Lifecycle: Persists locally; uploaded to dashboard; archived as GH Actions artifacts

**`.github/workflows/`:**
- Purpose: GitHub Actions CI/CD definitions
- Generated: No
- Committed: Yes
- Lifecycle: Executed by GitHub on schedule or manual trigger

**`.planning/`:**
- Purpose: Development planning and codebase analysis
- Generated: By codebase mapping tools
- Committed: Yes
- Lifecycle: Updated when architecture changes

---

*Structure analysis: 2026-01-18*
