# External Integrations

**Analysis Date:** 2026-01-18

## APIs & External Services

**Job Board Scraping (via python-jobspy):**
- Indeed - Job postings search
  - SDK/Client: `jobspy.scrape_jobs()`
  - Auth: None required (web scraping)
- ZipRecruiter - Job postings search
  - SDK/Client: `jobspy.scrape_jobs()`
  - Auth: None required (web scraping)
- Glassdoor - Job postings search
  - SDK/Client: `jobspy.scrape_jobs()`
  - Auth: None required (web scraping)

**Ops Dashboard:**
- Custom internal dashboard for lead management
  - SDK/Client: `requests` library, direct HTTP POST
  - Auth: Bearer token (`DASHBOARD_API_KEY` env var)
  - Endpoint: `{DASHBOARD_URL}/api/jobs/ingest`
  - Format: CSV payload with `Content-Type: text/csv`
  - Implementation: `upload_results.py`

**Google Search (URL Generation Only):**
- LinkedIn profile search URLs generated for manual use
  - No API integration - generates `google.com/search?q=` URLs
  - Used for: Finding managers, hiring contacts, role-specific people, end users
  - Implementation: `scraper.py` functions `generate_linkedin_*_url()`

## Data Storage

**Databases:**
- None (stateless scraper)

**File Storage:**
- Local filesystem only
  - Output directory: `output/`
  - File pattern: `solar_leads_{timestamp}.csv`
  - Generated by: `scraper.py`

**Caching:**
- None

## Authentication & Identity

**Auth Provider:**
- None for the scraper itself

**Dashboard Auth:**
- Bearer token authentication
- Token stored in GitHub Secrets (`DASHBOARD_API_KEY`)

## Monitoring & Observability

**Error Tracking:**
- None (errors print to stdout)

**Logs:**
- Console output only (`print()` statements)
- GitHub Actions logs capture all output

## CI/CD & Deployment

**Hosting:**
- GitHub Actions (serverless execution)
- No persistent infrastructure

**CI Pipeline:**
- GitHub Actions workflow: `.github/workflows/scrape-leads.yml`
- Schedule: Daily at 4am CT (10:00 UTC)
- Manual trigger: workflow_dispatch enabled
- Artifacts: CSV files retained for 30 days

## Environment Configuration

**Required env vars:**
- `DASHBOARD_URL` - Base URL for ops-dashboard (e.g., `https://dashboard.example.com`)
- `DASHBOARD_API_KEY` - API bearer token for authentication

**Secrets location:**
- GitHub repository secrets
- Local `.env` file (gitignored)

## Webhooks & Callbacks

**Incoming:**
- None

**Outgoing:**
- POST to `{DASHBOARD_URL}/api/jobs/ingest` after each scrape
  - Triggered by: `upload_results.py`
  - Payload: CSV content
  - Response: JSON with `count` of imported leads

---

*Integration audit: 2026-01-18*
