---
phase: 01-metrics-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - data/golden/golden-test-set.json
  - .planning/phases/01-metrics-foundation/01-BASELINE.md
autonomous: true

must_haves:
  truths:
    - "Running `python evaluate.py --golden` produces metrics output"
    - "Golden test set contains at least 30 items (16 positive + 14+ negative)"
    - "Baseline precision is documented with actual measured value"
    - "False positive categories are documented in baseline"
  artifacts:
    - path: "data/golden/golden-test-set.json"
      provides: "Curated regression test set"
      contains: "\"items\":"
      min_lines: 100
    - path: ".planning/phases/01-metrics-foundation/01-BASELINE.md"
      provides: "Documented baseline metrics"
      contains: "Precision"
  key_links:
    - from: "data/golden/golden-test-set.json"
      to: "evaluate.py"
      via: "--golden flag loads this file"
      pattern: "golden-test-set.json"
---

<objective>
Create the golden test set and document baseline filter performance.

Purpose: Establish a stable test set for regression testing (so future filter changes can be verified) and document the current baseline metrics (so we know if we're improving).

Output: Golden test set JSON file with curated examples, and BASELINE.md documenting current precision/recall.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/01-metrics-foundation/01-RESEARCH.md
@scraper.py
@evaluate.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create golden test set with curated examples</name>
  <files>data/golden/golden-test-set.json</files>
  <action>
    Create golden-test-set.json with curated examples from PROJECT.md patterns.

    **Schema:**
    ```json
    {
      "metadata": {
        "created": "2026-01-18",
        "purpose": "regression_testing",
        "criteria_version": "1.0",
        "notes": "Curated test set for filter evaluation. Do not modify frequently."
      },
      "items": [...]
    }
    ```

    **Required items (minimum 30 total):**

    **POSITIVE EXAMPLES (16+ items) - Should return True from filter:**

    Category: tier1_tool_match (solar-specific tools)
    - Solar Designer at SunPower using Helioscope for residential permit sets
    - PV System Designer at Soltage using Aurora Solar and PVsyst
    - Solar Design Engineer at EDP Renewables using PVsyst for utility-scale

    Category: tier2_strong_signals (stringing diagrams, permit sets)
    - AutoCAD Drafter creating stringing diagrams and single line diagrams for solar arrays
    - CAD Designer producing permit packages and plan sets for rooftop solar
    - Solar Designer creating module layouts and wiring schedules

    Category: tier3_cad_with_context (general CAD + solar project)
    - Electrical Designer using AutoCAD for commercial solar installations
    - CAD Technician working on residential solar permit sets

    Category: tier4_title_signals (explicit solar design titles)
    - "Solar Designer" role at Kimley-Horn doing PV system design
    - "PV Design Engineer" at Shoals Technologies

    Category: tier5_cad_design_role (CAD + design role + solar mentioned)
    - AutoCAD Designer at solar company working on array layouts

    Category: tier6_solar_design_titles (electrical designer at solar company)
    - Electrical Drafter at solar EPC company

    **NEGATIVE EXAMPLES (14+ items) - Should return False from filter:**

    Category: false_positive_tennis
    - Tennis racquet stringing technician (contains "stringing" but tennis context)

    Category: false_positive_aerospace
    - Electrical Engineer at SpaceX working on spacecraft solar panels
    - CAD Designer at Boeing working on satellite systems
    - Solar array engineer at Northrop Grumman for space systems

    Category: false_positive_semiconductor
    - ASIC Design Engineer at semiconductor company (RTL, synthesis)
    - CAD Engineer at chip company doing physical verification

    Category: false_positive_installer
    - Solar Installation Technician doing rooftop installs
    - PV Technician - field service role
    - Journeyman Electrician for solar installations

    Category: false_positive_sales
    - Solar Sales Engineer at residential solar company
    - Business Development Manager at solar company

    Category: false_positive_management
    - Solar Project Manager overseeing construction
    - Construction Manager at utility-scale solar site

    Category: false_positive_other_engineering
    - Civil Engineer on solar farm project (not design role)
    - Structural Engineer for solar carport

    Category: false_positive_utility
    - Transmission Engineer at utility company (solar mentioned)
    - Substation Engineer working on solar interconnection

    **Each item must include:**
    - id: Unique identifier (e.g., "golden_pos_01", "golden_neg_tennis_01")
    - description: Realistic job description text (50-200 words)
    - label: true or false
    - category: Category name from above
    - notes: Why this example is included (what it tests)

    **Important:** Descriptions must be realistic - look at actual job posting patterns. Include specific tool names, responsibilities, requirements sections like real postings.
  </action>
  <verify>
    - `python -c "import json; d=json.load(open('data/golden/golden-test-set.json')); print(f'Items: {len(d[\"items\"])}')"` shows 30+ items
    - `python evaluate.py --golden` runs without errors
    - JSON is valid: `python -m json.tool data/golden/golden-test-set.json > /dev/null`
  </verify>
  <done>
    - Golden test set exists at data/golden/golden-test-set.json
    - Contains 30+ items (16+ positive, 14+ negative)
    - Covers all documented false positive categories
    - Each item has id, description, label, category, notes
  </done>
</task>

<task type="auto">
  <name>Task 2: Run evaluation and document baseline metrics</name>
  <files>.planning/phases/01-metrics-foundation/01-BASELINE.md</files>
  <action>
    1. Run `python evaluate.py --golden --verbose` and capture output
    2. Create 01-BASELINE.md documenting the results

    **01-BASELINE.md structure:**

    ```markdown
    # Phase 1: Baseline Metrics

    **Measured:** [date]
    **Golden Test Set:** data/golden/golden-test-set.json ([N] items)

    ## Current Filter Performance

    | Metric | Value |
    |--------|-------|
    | Precision | X.XX% |
    | Recall | X.XX% |
    | F1 Score | X.XX |

    ## Confusion Matrix

    |                | Predicted Positive | Predicted Negative |
    |----------------|-------------------|-------------------|
    | Actual Positive | [TP] | [FN] |
    | Actual Negative | [FP] | [TN] |

    ## Analysis

    ### What's Working
    - [List categories where filter correctly classifies]

    ### False Positives (Predicted True, Actually False)
    - [List specific items that filter incorrectly passed]
    - [Note patterns]

    ### False Negatives (Predicted False, Actually True)
    - [List specific items that filter incorrectly rejected]
    - [Note patterns - these are missed good leads]

    ## Baseline for Comparison

    All future filter changes should be measured against this baseline:
    - Precision: X.XX% (goal: >20%)
    - Recall: X.XX% (goal: maintain or improve)

    ## Notes

    - Golden set size: [N] items ([P] positive, [N] negative)
    - Sample size limitation: With only [N] items, metrics have variance
    - Criteria version: 1.0

    ---
    *Baseline measured: [date]*
    ```

    3. If precision is not ~3% as expected, note the discrepancy and investigate (the golden set may differ from production data distribution)
  </action>
  <verify>
    - File exists: `ls .planning/phases/01-metrics-foundation/01-BASELINE.md`
    - Contains precision value: `grep -i "precision" .planning/phases/01-metrics-foundation/01-BASELINE.md`
    - Contains confusion matrix: `grep -i "confusion" .planning/phases/01-metrics-foundation/01-BASELINE.md`
  </verify>
  <done>
    - 01-BASELINE.md exists with measured metrics
    - Precision and recall values documented
    - Confusion matrix included
    - False positive/negative analysis included
    - Baseline values stated for future comparison
  </done>
</task>

</tasks>

<verification>
Run full verification:
1. `python evaluate.py --golden` completes successfully
2. `cat .planning/phases/01-metrics-foundation/01-BASELINE.md` shows metrics
3. Golden test set has correct structure:
   ```bash
   python -c "
   import json
   with open('data/golden/golden-test-set.json') as f:
       data = json.load(f)
   items = data['items']
   pos = sum(1 for i in items if i['label'])
   neg = len(items) - pos
   print(f'Total: {len(items)}, Positive: {pos}, Negative: {neg}')
   assert len(items) >= 30, 'Need at least 30 items'
   assert pos >= 16, 'Need at least 16 positive examples'
   assert neg >= 14, 'Need at least 14 negative examples'
   print('Golden test set validation: PASSED')
   "
   ```
</verification>

<success_criteria>
- Golden test set exists with 30+ curated items
- Running `python evaluate.py --golden` produces metrics
- 01-BASELINE.md documents precision, recall, F1, confusion matrix
- False positive categories identified and documented
- Baseline provides comparison point for future phases
</success_criteria>

<output>
After completion, create `.planning/phases/01-metrics-foundation/01-02-SUMMARY.md`
</output>
