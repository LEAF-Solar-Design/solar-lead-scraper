---
phase: 01-metrics-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - data/labeled/.gitkeep
  - data/golden/.gitkeep
  - evaluate.py
autonomous: true

must_haves:
  truths:
    - "Running `python evaluate.py --help` shows usage instructions"
    - "Running `python evaluate.py` against labeled JSON produces precision/recall numbers"
    - "Evaluation script handles empty files and missing fields gracefully"
  artifacts:
    - path: "requirements.txt"
      provides: "scikit-learn dependency"
      contains: "scikit-learn"
    - path: "evaluate.py"
      provides: "Evaluation script with CLI interface"
      exports: ["load_labeled_data", "evaluate", "main"]
      min_lines: 80
    - path: "data/labeled/.gitkeep"
      provides: "Directory for labeled data files"
    - path: "data/golden/.gitkeep"
      provides: "Directory for golden test set"
  key_links:
    - from: "evaluate.py"
      to: "scraper.py"
      via: "import description_matches"
      pattern: "from scraper import description_matches"
    - from: "evaluate.py"
      to: "sklearn.metrics"
      via: "import precision_score, recall_score"
      pattern: "from sklearn.metrics import"
---

<objective>
Create the evaluation infrastructure for measuring filter precision and recall.

Purpose: Establish the measurement foundation so all future filter improvements can be tracked and compared. Without metrics, we can't know if changes help or hurt.

Output: Working evaluate.py script that loads labeled JSON data and computes precision/recall against the existing description_matches filter.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/01-metrics-foundation/01-RESEARCH.md
@scraper.py
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add scikit-learn dependency and create data directories</name>
  <files>requirements.txt, data/labeled/.gitkeep, data/golden/.gitkeep</files>
  <action>
    1. Add `scikit-learn` to requirements.txt (after pandas)
    2. Create data/labeled/ directory with .gitkeep file
    3. Create data/golden/ directory with .gitkeep file

    The data directories will hold:
    - data/labeled/ - JSON files with labeled leads for development/analysis
    - data/golden/ - JSON file(s) for regression testing (stable test set)
  </action>
  <verify>
    - `cat requirements.txt` shows scikit-learn
    - `ls data/labeled/.gitkeep` exists
    - `ls data/golden/.gitkeep` exists
  </verify>
  <done>
    - requirements.txt contains scikit-learn
    - data/labeled/ directory exists
    - data/golden/ directory exists
  </done>
</task>

<task type="auto">
  <name>Task 2: Create evaluate.py evaluation script</name>
  <files>evaluate.py</files>
  <action>
    Create evaluate.py following the pattern from 01-RESEARCH.md with these specifications:

    **CLI Interface:**
    ```
    python evaluate.py                    # Evaluate against all files in data/labeled/
    python evaluate.py --golden           # Evaluate against data/golden/golden-test-set.json only
    python evaluate.py --file PATH        # Evaluate against specific file
    python evaluate.py --verbose          # Show per-item results
    ```

    **Labeled Data Schema (must support):**
    ```json
    {
      "metadata": {
        "created": "2026-01-18",
        "source": "manual_labeling",
        "criteria_version": "1.0"
      },
      "items": [
        {
          "id": "job_123",
          "description": "Solar Designer using Helioscope...",
          "label": true,
          "company": "SunPower",
          "title": "Solar Designer",
          "notes": "Clear solar design role"
        }
      ]
    }
    ```
    Also support raw array format (just items without wrapper).

    **Functions to implement:**

    1. `load_labeled_data(filepath: Path) -> list[dict]`
       - Load JSON file
       - Handle both wrapped (with metadata) and raw array formats
       - Validate required fields: description, label (boolean)
       - Raise ValueError with clear message if validation fails

    2. `evaluate(items: list[dict], verbose: bool = False) -> dict`
       - Run description_matches() on each item's description
       - Compute metrics using sklearn:
         - precision_score(y_true, y_pred, zero_division=0)
         - recall_score(y_true, y_pred, zero_division=0)
         - f1_score(y_true, y_pred, zero_division=0)
       - Return dict with: precision, recall, f1, total, positives, negatives, true_positives, false_positives, false_negatives, true_negatives
       - If verbose, print each item result (id, expected, predicted, match/mismatch)

    3. `print_report(metrics: dict, source: str)`
       - Pretty-print metrics in readable format
       - Include confusion matrix visualization
       - Show source file path

    4. `main()`
       - Parse CLI arguments using argparse
       - Load file(s) based on --golden, --file, or default (all in data/labeled/)
       - Call evaluate() and print_report()
       - Exit with code 0 on success, 1 on error

    **Import from scraper.py:**
    ```python
    from scraper import description_matches
    ```

    **Error handling:**
    - FileNotFoundError: Clear message about missing file
    - JSONDecodeError: Clear message about invalid JSON
    - KeyError/ValueError: Clear message about missing/invalid fields
    - Empty results: Warn but don't crash
  </action>
  <verify>
    - `python evaluate.py --help` shows usage
    - Script imports successfully: `python -c "import evaluate"`
    - Script handles missing file gracefully: `python evaluate.py --file nonexistent.json` shows error, exits 1
  </verify>
  <done>
    - evaluate.py exists with CLI interface
    - evaluate.py imports description_matches from scraper
    - evaluate.py uses sklearn for metrics
    - evaluate.py validates labeled data schema
    - Running --help shows all options
  </done>
</task>

</tasks>

<verification>
Run verification sequence:
1. `pip install -r requirements.txt` completes without errors
2. `python evaluate.py --help` shows usage with --golden, --file, --verbose options
3. `python -c "from evaluate import load_labeled_data, evaluate; print('OK')"` prints OK
4. Create a minimal test file and verify evaluation works:
   ```bash
   echo '{"items": [{"id": "test1", "description": "Solar Designer using Helioscope", "label": true}]}' > data/labeled/test.json
   python evaluate.py --file data/labeled/test.json
   rm data/labeled/test.json
   ```
</verification>

<success_criteria>
- evaluate.py exists and runs without errors
- CLI supports --golden, --file, and --verbose flags
- Evaluation produces precision/recall/f1 metrics
- Script imports description_matches from existing scraper.py
- Data directories exist for future labeled data files
</success_criteria>

<output>
After completion, create `.planning/phases/01-metrics-foundation/01-01-SUMMARY.md`
</output>
