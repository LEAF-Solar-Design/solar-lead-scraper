---
phase: 04-quality-instrumentation
plan: 02
type: execute
wave: 2
depends_on: [04-01]
files_modified:
  - scraper.py
autonomous: true

must_haves:
  truths:
    - "User can find rejected leads JSON file in output directory after run"
    - "Rejected leads file follows labeled data schema (id, description, label, company, title)"
    - "CSV output includes confidence_score column for qualified leads"
    - "Confidence scores range from 0-100 based on scoring result"
  artifacts:
    - path: "scraper.py"
      provides: "export_rejected_leads function"
      contains: "def export_rejected_leads"
    - path: "scraper.py"
      provides: "Confidence score in CSV output"
      contains: "confidence_score"
    - path: "output/rejected_leads_*.json"
      provides: "Rejected leads for labeling"
      min_lines: 10
  key_links:
    - from: "scrape_solar_jobs()"
      to: "export_rejected_leads()"
      via: "rejected lead collection during filtering"
      pattern: "export_rejected_leads\\("
    - from: "process_jobs()"
      to: "ScoringResult.score"
      via: "confidence score calculation"
      pattern: "confidence_score"
---

<objective>
Add rejected lead export for labeling (QUAL-02) and confidence scores in output (QUAL-03).

Purpose: Enable continuous improvement workflow - export rejected leads for human review to find false negatives, and surface confidence in qualified leads for prioritization.

Output: export_rejected_leads() function producing JSON matching labeled data schema, confidence_score column in CSV output, and wiring in main() to generate both outputs.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-quality-instrumentation/04-RESEARCH.md
@.planning/phases/04-quality-instrumentation/04-01-SUMMARY.md (if exists, for FilterStats context)

# Key existing code
@scraper.py (ScoringResult, score_job, scrape_solar_jobs now returns tuple with FilterStats)
@data/golden/golden-test-set.json (schema reference for rejected export format)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add export_rejected_leads function</name>
  <files>scraper.py</files>
  <action>
Add export_rejected_leads() function to scraper.py. This function exports rejected leads in a format matching the golden test set schema for easy import into labeling workflow.

```python
def export_rejected_leads(
    rejected_leads: list[dict],
    output_dir: Path,
    run_id: str,
    max_export: int = 100
) -> Path:
    """Export rejected leads for labeling review.

    Exports rejected leads in the same schema as golden-test-set.json
    for easy import into labeling workflow after human review.

    Args:
        rejected_leads: List of dicts with keys: id, description, company, title, rejection_reason, score
        output_dir: Directory to write JSON file
        run_id: Timestamp or identifier for this run
        max_export: Maximum leads to export (default 100 to avoid huge files)

    Returns:
        Path to the created file
    """
    # Limit export size
    to_export = rejected_leads[:max_export]

    # Convert to labeled data schema
    items = []
    for lead in to_export:
        item = {
            "id": lead.get("id", f"rejected_{len(items)+1:03d}"),
            "description": lead.get("description", "")[:2000],  # Truncate long descriptions
            "label": False,  # Presumed false, reviewer confirms or changes to true
            "company": lead.get("company"),
            "title": lead.get("title"),
            "notes": f"Rejected: {lead.get('rejection_reason', 'unknown')} (score: {lead.get('score', 0)})"
        }
        items.append(item)

    export_data = {
        "metadata": {
            "created": datetime.now().isoformat(),
            "purpose": "labeling_review",
            "run_id": run_id,
            "count": len(items),
            "total_rejected": len(rejected_leads),
            "notes": "Review and change label to true for any false negatives"
        },
        "items": items
    }

    filepath = output_dir / f"rejected_leads_{run_id}.json"
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, indent=2)

    return filepath
```
  </action>
  <verify>
Test function in isolation:
```bash
python -c "
from scraper import export_rejected_leads
from pathlib import Path
import tempfile

leads = [
    {'id': 'test_01', 'description': 'Test job', 'company': 'TestCo', 'title': 'Engineer', 'rejection_reason': 'no_solar_context', 'score': 0},
    {'id': 'test_02', 'description': 'Another test', 'company': 'OtherCo', 'title': 'Designer', 'rejection_reason': 'company_blocklist', 'score': -100}
]
with tempfile.TemporaryDirectory() as tmpdir:
    path = export_rejected_leads(leads, Path(tmpdir), '20260118_test')
    with open(path) as f:
        content = f.read()
    print('File created:', path.name)
    print('Contains items:', '\"items\"' in content)
    print('Contains metadata:', '\"metadata\"' in content)
"
```
Should print file created, items present, metadata present.
  </verify>
  <done>export_rejected_leads() function exists and produces JSON matching labeled data schema.</done>
</task>

<task type="auto">
  <name>Task 2: Collect rejected leads and add confidence to scrape_solar_jobs</name>
  <files>scraper.py</files>
  <action>
Modify scrape_solar_jobs() to collect rejected lead data during filtering. This builds on Plan 04-01's filtering loop.

Update the function signature to return three items:
```python
def scrape_solar_jobs() -> tuple[pd.DataFrame, FilterStats, list[dict]]:
```

In the filtering loop (added in 04-01), collect rejected lead data:

```python
# Before the loop
rejected_leads = []
scoring_results = {}  # Map row index to ScoringResult for confidence calculation

# In the loop, after stats.add_rejected():
if not result.qualified:
    # Collect rejected lead for export
    rejected_lead = {
        "id": f"rejected_{len(rejected_leads)+1:03d}_{row.get('company', 'unknown')[:20]}",
        "description": row['description'] if pd.notna(row.get('description')) else "",
        "company": row.get('company'),
        "title": row.get('title'),
        "rejection_reason": categorize_rejection(result),
        "score": result.score
    }
    rejected_leads.append(rejected_lead)
else:
    # Store scoring result for confidence calculation later
    scoring_results[idx] = result
```

Update return statement:
```python
return df, stats, rejected_leads
```

Also update the empty return case:
```python
return pd.DataFrame(), FilterStats(), []
```

Store scoring_results somewhere accessible for process_jobs. Options:
1. Return it as fourth tuple element (cleanest)
2. Store as module-level variable (simpler for this case)

Use option 1 - return scoring_results dict:
```python
def scrape_solar_jobs() -> tuple[pd.DataFrame, FilterStats, list[dict], dict]:
    # ...
    return df, stats, rejected_leads, scoring_results
```
  </action>
  <verify>
Verify function signature and return types:
```bash
python -c "
from scraper import scrape_solar_jobs
import inspect
sig = inspect.signature(scrape_solar_jobs)
print('Return annotation:', sig.return_annotation)
"
```
Should show tuple with DataFrame, FilterStats, list, dict.
  </verify>
  <done>scrape_solar_jobs() collects rejected leads and returns (DataFrame, FilterStats, rejected_leads, scoring_results).</done>
</task>

<task type="auto">
  <name>Task 3: Add confidence_score to process_jobs and wire main</name>
  <files>scraper.py</files>
  <action>
Update process_jobs() to accept scoring_results and add confidence_score column:

```python
def process_jobs(df: pd.DataFrame, scoring_results: dict = None) -> pd.DataFrame:
    """Process and dedupe jobs, extract company info.

    Args:
        df: DataFrame of job listings
        scoring_results: Optional dict mapping row indices to ScoringResult for confidence
    """
    if df.empty:
        return df

    # Keep relevant columns
    columns_to_keep = ['company', 'title', 'location', 'job_url']
    available_cols = [c for c in columns_to_keep if c in df.columns]
    df = df[available_cols].copy()

    # Add confidence score before deduplication (while we still have original indices)
    if scoring_results:
        def get_confidence(idx):
            result = scoring_results.get(idx)
            if result and result.qualified:
                # Score 50 = threshold (50% confidence), 100+ = 100% confidence
                return min(100.0, result.score)
            return None
        df['confidence_score'] = df.index.map(get_confidence)
    else:
        df['confidence_score'] = None

    # ... rest of existing code (dedupe, domain guess, etc.) ...
```

Update the final_columns list to include confidence_score:
```python
final_columns = ['company', 'domain', 'job_title', 'location', 'confidence_score', 'posting_url', 'linkedin_managers', 'linkedin_hiring', 'linkedin_role', 'google_enduser', 'date_scraped']
```

Update main() to:
1. Receive all four returns from scrape_solar_jobs()
2. Pass scoring_results to process_jobs()
3. Call export_rejected_leads() before saving CSV

```python
def main():
    print("=" * 50)
    print("Solar Job Lead Scraper")
    print("=" * 50)
    print()

    # Scrape jobs
    raw_jobs, stats, rejected_leads, scoring_results = scrape_solar_jobs()

    if raw_jobs.empty:
        print("No jobs to process. Exiting.")
        # Still print stats even if empty
        print_filter_stats(stats)
        return

    # Process and dedupe
    leads = process_jobs(raw_jobs, scoring_results)

    if leads.empty:
        print("No leads after processing. Exiting.")
        print_filter_stats(stats)
        return

    # Print filter statistics
    print_filter_stats(stats)

    # Ensure output directory exists
    output_dir = Path(__file__).parent / "output"
    output_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # Export rejected leads for labeling review
    if rejected_leads:
        rejected_path = export_rejected_leads(rejected_leads, output_dir, timestamp)
        print(f"\nExported {min(100, len(rejected_leads))} rejected leads to: {rejected_path}")

    # Save qualified leads to CSV
    output_file = output_dir / f"solar_leads_{timestamp}.csv"
    leads.to_csv(output_file, index=False)

    print()
    print("=" * 50)
    print(f"Saved {len(leads)} leads to: {output_file}")
    print("=" * 50)

    # Preview first few
    print("\nPreview of leads:")
    print(leads.head(10).to_string(index=False))
```
  </action>
  <verify>
Run evaluation to ensure no regressions:
```bash
python evaluate.py data/golden/golden-test-set.json
```
Expected: 100% precision, 75% recall (unchanged)

Verify CSV includes confidence_score column (would need actual run or mock).
  </verify>
  <done>process_jobs() adds confidence_score column, main() exports rejected leads and includes confidence in CSV.</done>
</task>

</tasks>

<verification>
Overall phase verification:
1. export_rejected_leads() function exists and produces valid JSON
2. Rejected leads collected during scrape_solar_jobs() filtering
3. Rejected leads export matches golden-test-set.json schema
4. process_jobs() accepts scoring_results and adds confidence_score
5. CSV output includes confidence_score column
6. main() calls export_rejected_leads() and includes scoring in process_jobs

Run evaluation to verify no regressions:
```bash
python evaluate.py data/golden/golden-test-set.json
```
Expected: 100% precision, 75% recall (unchanged from Phase 3)

Test rejected export schema compatibility:
```bash
python -c "
import json
with open('data/golden/golden-test-set.json') as f:
    golden = json.load(f)
print('Golden schema keys:', list(golden['items'][0].keys()))
# Should match: id, description, label, category, notes (optional: company, title)
"
```
</verification>

<success_criteria>
- [ ] export_rejected_leads() function exists with correct signature
- [ ] Rejected leads JSON matches labeled data schema (id, description, label, company, title, notes)
- [ ] Rejected leads limited to configurable max (default 100)
- [ ] scrape_solar_jobs() returns tuple with rejected_leads list
- [ ] process_jobs() accepts scoring_results parameter
- [ ] CSV output includes confidence_score column
- [ ] Confidence ranges 0-100 (score capped at 100)
- [ ] main() exports rejected leads to output/rejected_leads_*.json
- [ ] All existing tests pass (evaluate.py)
- [ ] QUAL-02 requirement satisfied: "Rejected leads can be exported for labeling"
- [ ] QUAL-03 requirement satisfied: "Qualified leads include confidence score"
</success_criteria>

<output>
After completion, create `.planning/phases/04-quality-instrumentation/04-02-SUMMARY.md`
</output>
