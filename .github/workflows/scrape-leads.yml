name: Scrape Solar Leads

on:
  # Run daily at 4am CT (10:00 UTC)
  schedule:
    - cron: '0 10 * * *'

  # Allow manual trigger from GitHub Actions tab
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        batch: [0, 1, 2, 3]  # 4 parallel runners, each with different IP
      fail-fast: false  # Continue other batches even if one fails

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install requests nodriver

      - name: Set up Chrome and Xvfb for browser scraping
        run: |
          # Install Chrome
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo gpg --dearmor -o /usr/share/keyrings/google-chrome.gpg
          echo "deb [arch=amd64 signed-by=/usr/share/keyrings/google-chrome.gpg] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable xvfb
          # Verify Chrome installed
          google-chrome --version

      - name: Run scraper (batch ${{ matrix.batch }})
        env:
          SCRAPER_PROXIES: ${{ secrets.SCRAPER_PROXIES }}
          SCRAPER_BATCH: ${{ matrix.batch }}
          SCRAPER_TOTAL_BATCHES: 4
          ENABLE_BROWSER_SCRAPING: "1"
          DISPLAY: ":99"
        run: |
          # Start Xvfb virtual display for browser automation
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 2
          # Run the scraper
          python scraper.py

      - name: Upload batch results
        uses: actions/upload-artifact@v4
        if: always()  # Upload even if scraper had partial failures
        with:
          name: batch-${{ matrix.batch }}
          path: |
            output/*.csv
            output/*.json
          retention-days: 1
          if-no-files-found: ignore  # Don't fail if no files produced

  merge:
    needs: scrape
    if: always()  # Run merge even if some batches failed
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests

      - name: Download all batch artifacts
        uses: actions/download-artifact@v4
        with:
          path: batches/
          pattern: batch-*

      - name: Merge batch results
        run: |
          python -c "
          import json
          import pandas as pd
          from pathlib import Path
          from datetime import datetime

          output_dir = Path('output')
          output_dir.mkdir(exist_ok=True)
          timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

          # Find all CSV files from batches
          csv_files = list(Path('batches').glob('**/solar_leads_*.csv'))
          print(f'Found {len(csv_files)} batch CSV files')

          if csv_files:
              # Read and combine all CSVs
              dfs = [pd.read_csv(f) for f in csv_files]
              combined = pd.concat(dfs, ignore_index=True)
              print(f'Combined: {len(combined)} total rows')

              # Dedupe by company (keep first occurrence)
              combined = combined.drop_duplicates(subset=['company'], keep='first')
              print(f'After deduping: {len(combined)} unique companies')

              # Save merged output
              output_file = output_dir / f'solar_leads_{timestamp}.csv'
              combined.to_csv(output_file, index=False)
              print(f'Saved merged leads to: {output_file}')
          else:
              print('No CSV files found')

          # Find and merge all search error JSON files
          error_files = list(Path('batches').glob('**/search_errors_*.json'))
          print(f'Found {len(error_files)} search error files')

          if error_files:
              all_errors = []
              total_by_type = {}

              for filepath in error_files:
                  with open(filepath) as f:
                      data = json.load(f)
                  all_errors.extend(data.get('errors', []))
                  for error_type, count in data.get('metadata', {}).get('error_summary', {}).items():
                      total_by_type[error_type] = total_by_type.get(error_type, 0) + count

              merged_errors = {
                  'metadata': {
                      'created': datetime.now().isoformat(),
                      'run_id': timestamp,
                      'total_errors': len(all_errors),
                      'error_summary': total_by_type,
                      'source_files': len(error_files)
                  },
                  'errors': all_errors
              }

              error_output = output_dir / f'search_errors_{timestamp}.json'
              with open(error_output, 'w') as f:
                  json.dump(merged_errors, f, indent=2)
              print(f'Saved merged search errors to: {error_output}')
              print(f'Error summary: {total_by_type}')
          "

      - name: Upload results to dashboard
        env:
          DASHBOARD_URL: ${{ secrets.DASHBOARD_URL }}
          DASHBOARD_API_KEY: ${{ secrets.DASHBOARD_API_KEY }}
        run: python upload_results.py

      - name: Upload merged results as artifact (backup)
        uses: actions/upload-artifact@v4
        with:
          name: solar-leads-${{ github.run_number }}
          path: |
            output/*.csv
            output/*.json
          retention-days: 30
