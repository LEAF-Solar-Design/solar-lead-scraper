name: Scrape Solar Leads

on:
  # Run daily at 2am CT (8:00 UTC)
  schedule:
    - cron: '0 8 * * *'

  # Allow manual trigger from GitHub Actions tab
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # Kill hung jobs - quadrupled from 45 min
    strategy:
      matrix:
        batch: [0, 1, 2, 3]  # 4 parallel runners, each with different IP
      fail-fast: false  # Continue other batches even if one fails

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Cache Camoufox browser
        id: cache-camoufox
        uses: actions/cache@v4
        with:
          path: ~/.camoufox
          key: camoufox-${{ runner.os }}-v2

      - name: Set up Camoufox browser
        if: steps.cache-camoufox.outputs.cache-hit != 'true'
        run: |
          # Download the Camoufox Firefox-based browser binary
          # Camoufox is better for CI - works headless without Xvfb issues
          # Only one job will download, others will use cache

          # Stagger matrix jobs to avoid simultaneous GitHub API hits
          # Each batch waits (batch_number * 15) seconds before starting
          sleep $(( ${{ matrix.batch }} * 15 ))

          # Patch camoufox to use GITHUB_TOKEN (not yet in released version)
          # See: https://github.com/daijro/camoufox/issues/285
          python .github/scripts/patch_camoufox.py

          # Retry with backoff to handle rate limits
          for i in 1 2 3 4 5; do
            if camoufox fetch; then
              echo "Camoufox browser downloaded successfully"
              break
            else
              if [ $i -lt 5 ]; then
                wait_time=$((i * 30))
                echo "Download failed, retrying in ${wait_time}s (attempt $i/5)..."
                sleep $wait_time
              else
                echo "Failed to download Camoufox after 5 attempts"
                exit 1
              fi
            fi
          done
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Run scraper (batch ${{ matrix.batch }})
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 160
          max_attempts: 3
          retry_wait_seconds: 30
          command: python scraper.py
        env:
          SCRAPER_PROXIES: ${{ secrets.SCRAPER_PROXIES }}
          SCRAPER_BATCH: ${{ matrix.batch }}
          SCRAPER_TOTAL_BATCHES: 4
          # Enable Camoufox browser scraping for ZipRecruiter/Glassdoor
          # Camoufox uses Firefox with anti-detect patches, works better in CI than Chrome
          ENABLE_BROWSER_SCRAPING: "1"
          # Enable debug screenshots to diagnose selector issues
          CAMOUFOX_DEBUG: "1"

      - name: Upload batch results
        uses: actions/upload-artifact@v4
        if: always()  # Upload even if scraper had partial failures
        with:
          name: batch-${{ matrix.batch }}
          path: |
            output/*.csv
            output/*.json
          retention-days: 1
          if-no-files-found: ignore  # Don't fail if no files produced

      - name: Upload debug screenshots
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: debug-screenshots-batch-${{ matrix.batch }}
          path: output/debug_screenshots/
          retention-days: 7
          if-no-files-found: ignore

  merge:
    needs: scrape
    if: always()  # Run merge even if some batches failed
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests

      - name: Download all batch artifacts
        uses: actions/download-artifact@v4
        with:
          path: batches/
          pattern: batch-*

      - name: Merge batch results
        run: python .github/scripts/merge_batch_results.py

      - name: Upload results to dashboard
        env:
          DASHBOARD_URL: ${{ secrets.DASHBOARD_URL }}
          DASHBOARD_API_KEY: ${{ secrets.DASHBOARD_API_KEY }}
        run: python upload_results.py

      - name: Upload merged results as artifact (backup)
        uses: actions/upload-artifact@v4
        with:
          name: solar-leads-${{ github.run_number }}
          path: |
            output/*.csv
            output/*.json
          retention-days: 30
